# üìä Um Bilh√£o de Linhas: Desafio de Processamento de Dados com Python

## üöÄ Vis√£o Geral

Este projeto tem como objetivo mostrar como **processar um arquivo massivo de 1 bilh√£o de linhas (~14GB)** usando Python e bibliotecas modernas para extrair estat√≠sticas como temperatura m√≠nima, m√©dia e m√°xima por esta√ß√£o meteorol√≥gica ‚Äî tudo isso com foco em efici√™ncia e performance.

Inspirado no famoso [One Billion Row Challenge](https://github.com/gunnarmorling/1brc), originalmente proposto em Java, a proposta aqui √© mostrar como Python tamb√©m pode resolver este tipo de tarefa.

## üìì Estrutura do Arquivo

Cada linha do arquivo representa uma medi√ß√£o de temperatura de uma esta√ß√£o meteorol√≥gica, no formato:
```
<string: nome da esta√ß√£o>;<double: medi√ß√£o>
```

Exemplo de 10 linhas:
```
Hamburg;12.0
Bulawayo;8.9
Palembang;38.8
St. Johns;15.2
Cracow;12.6
Bridgetown;26.9
Istanbul;6.2
Roseau;34.4
Conakry;31.2
Istanbul;23.0
```

## üéØ Objetivo

Para cada esta√ß√£o, calcular:
- **Temperatura m√≠nima**
- **Temperatura m√©dia** (arredondada para uma casa decimal)
- **Temperatura m√°xima**

Os resultados devem ser exibidos em uma tabela ordenada pelo nome da esta√ß√£o:

| station      | min_temperature | mean_temperature | max_temperature |
|--------------|-----------------|------------------|-----------------|
| Abha         | -31.1           | 18.0             | 66.5            |
| Abidjan      | -25.9           | 26.0             | 74.6            |
| Ab√©ch√©       | -19.8           | 29.4             | 79.9            |
| Accra        | -24.8           | 26.4             | 76.3            |
| Addis Ababa  | -31.8           | 16.0             | 63.9            |
| Adelaide     | -31.8           | 17.3             | 71.5            |
| Aden         | -19.6           | 29.1             | 78.3            |
| Ahvaz        | -24.0           | 25.4             | 72.6            |
| Albuquerque  | -35.0           | 14.0             | 61.9            |
| Alexandra    | -40.1           | 11.0             | 67.9            |
| ...          | ...             | ...              | ...             |
| Yangon       | -23.6           | 27.5             | 77.3            |
| Yaound√©      | -26.2           | 23.8             | 73.4            |
| Yellowknife  | -53.4           | -4.3             | 46.7            |
| Yerevan      | -38.6           | 12.4             | 62.8            |
| Yinchuan     | -45.2           | 9.0              | 56.9            |
| Zagreb       | -39.2           | 10.7             | 58.1            |
| Zanzibar City| -26.5           | 26.0             | 75.2            |
| Z√ºrich       | -42.0           | 9.3              | 63.6            |
| √úr√ºmqi       | -42.1           | 7.4              | 56.7            |
| ƒ∞zmir        | -34.4           | 17.9             | 67.9            |

## üß∞ Depend√™ncias

Para executar os scripts deste projeto, voc√™ precisar√° das seguintes bibliotecas:

* Polars: `0.20.3`
* DuckDB: `0.10.0`
* Dask[complete]: `^2024.2.0`

## ‚öôÔ∏è Como Executar



Para executar este projeto e reproduzir os resultados:

1. Clone o reposit√≥rio

2. Defina o Python com `pyenv local 3.12.1`

3. Configure o ambiente:
```
poetry env use 3.12.1
poetry install --no-root
poetry lock
```

4. Gere o arquivo de testes:
```
python src/create_measurements.py
```
_(A gera√ß√£o do arquivo pode levar cerca de 10 minutos.)_

5. Certifique-se de instalar as vers√µes especificadas das bibliotecas Dask, Polars e DuckDB

6. Execute os scripts com:
```
poetry run python src/using_python.py
poetry run python src/using_pandas.py
poetry run python src/using_dask.py
poetry run python src/using_polars.py
poetry run python src/using_duckdb.py` 
```

## Resultados

Os testes foram realizados em um laptop equipado com um processador M1 da Apple e 8GB de RAM. As implementa√ß√µes utilizaram abordagens puramente Python, Pandas, Dask, Polars e DuckDB. Os resultados de tempo de execu√ß√£o para processar o arquivo de 1 bilh√£o de linhas s√£o apresentados abaixo:

| Implementa√ß√£o | Tempo |
| --- | --- |
| Bash + awk | 25 minutos |
| Python | 20 minutos |
| Python + Pandas | 263 sec |
| Python + Dask | 155.62 sec  |
| Python + Polars | 33.86 sec |
| Python + Duckdb | 14.98 sec |

## üîß B√¥nus

Verifique se as ferramentas utilizadas no script (`wc`, `head`, `pv`, `awk`, e `sort`) est√£o instaladas em seu sistema. Se necess√°rio instale `pv` (Pipe Viewer) pode precisar ser instalado manualmente.

### Instalando o Pipe Viewer (pv)

Se voc√™ n√£o tem o `pv` instalado, pode facilmente instal√°-lo usando o gerenciador de pacotes do seu sistema. Por exemplo:

* No Ubuntu/Debian:
    
    ```bash
    sudo apt-get update
    sudo apt-get install pv
    ```
    
* No macOS (usando [Homebrew](https://brew.sh/)):
    
    ```bash
    brew install pv
    ```
    
### Preparando o Script

1. D√™ permiss√£o de execu√ß√£o para o arquivo script. Abra um terminal e execute:
    
    ```bash
    chmod +x process_measurements.sh
    ```

2. Rode o script. Abra um terminal e execute:
   
   ```bash
   ./src/using_bash_and_awk.sh 1000
   ```

Neste exemplo, apenas as primeiras 1000 linhas ser√£o processadas.

Ao executar o script, voc√™ ver√° a barra de progresso (se pv estiver instalado corretamente) e, eventualmente, a sa√≠da esperada no terminal ou em um arquivo de sa√≠da, se voc√™ decidir modificar o script para direcionar a sa√≠da.

## üß† Conclus√£o

Este desafio destacou claramente a efic√°cia de diversas bibliotecas Python na manipula√ß√£o de grandes volumes de dados. M√©todos tradicionais como Bash (25 minutos), Python puro (20 minutos) e at√© mesmo o Pandas (5 minutos) demandaram uma s√©rie de t√°ticas para implementar o processamento em "lotes", enquanto bibliotecas como Dask, Polars e DuckDB provaram ser excepcionalmente eficazes, requerendo menos linhas de c√≥digo devido √† sua capacidade inerente de distribuir os dados em "lotes em streaming" de maneira mais eficiente. O DuckDB se sobressaiu, alcan√ßando o menor tempo de execu√ß√£o gra√ßas √† sua estrat√©gia de execu√ß√£o e processamento de dados.

Esses resultados enfatizam a import√¢ncia de selecionar a ferramenta adequada para an√°lise de dados em larga escala, demonstrando que Python, com as bibliotecas certas, √© uma escolha poderosa para enfrentar desafios de big data.

## Agradecimentos

- Obrigado:
    - [Luciano](https://suajornadadedados.com.br/) pela aula no JD
    - [Koen Vossen](https://github.com/koenvo) pela implementa√ß√£o em Polars
    - [Arthur Juli√£o](https://github.com/ArthurJ) pela implementa√ß√£o em Python e Bash 